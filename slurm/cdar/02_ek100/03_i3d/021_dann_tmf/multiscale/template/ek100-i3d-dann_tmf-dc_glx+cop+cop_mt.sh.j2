#!/bin/bash

#SBATCH -J {{ idx }}_{{ source }}_{{ target }}-i3d-dann_tmf-dc_glx+cop+cop_mt
#SBATCH -p batch_grad
#SBATCH --gres=gpu:4
#SBATCH --cpus-per-gpu=8
#SBATCH --mem-per-gpu=29G
#SBATCH -t 1-0
#SBATCH -x ariel-g[1-5],ariel-v[1-3,5,13]
#SBATCH --array 0-3%4
#SBATCH -o slurm/logs/slurm-%A_%a-%x.out

current_time=$(date +'%Y%m%d-%H%M%S')


# overleaf table 기준
project='cdar'
task='02_ek100'  # table name
subtask={{ idx }}_closed_{{ source }}_{{ target }}  # column
backbone='03_i3d'
model='021_dann_tmf'  # row
add_on='multiscale'
extra_setting='dc_glx+cop+cop_mt'  # 'default' if none
path_experiment="${project}/${task}/${subtask}/${backbone}/${model}/${add_on}/${extra_setting}"

workdir="work_dirs/train_output"
workdir="${workdir}/${path_experiment}"
workdir="${workdir}/${SLURM_ARRAY_JOB_ID}__${SLURM_JOB_NAME}/${SLURM_ARRAY_TASK_ID}/${current_time}"

lr=1e-3

config_j2='configs/recognition/cdar/02_ek100/03_i3d/021_dann_tmf/multiscale/template/ek100_i3d_dann_tmf_lngn_cop.py.j2'
rendered_dir="${config_j2%template*}rendered"  # **/template/* -> **/rendered
mkdir -p $rendered_dir
config="${rendered_dir}/{{ source }}_{{ target }}_$(basename $config_j2 .j2)"
python slurm/utils/commons/render_template.py -f $config_j2 --source {{ source }} --target {{ target }} > $config

ckpts=(
    work_dirs/train_output/cdar/02_ek100/00_D2_D1/03_i3d/011_cop_pretrain/default/default/17929__00_D2_D1-i3d-cop_pretrain/0/20230615-042104/epoch_200.pth
    work_dirs/train_output/cdar/02_ek100/01_D3_D1/03_i3d/011_cop_pretrain/default/default/17930__01_D3_D1-i3d-cop_pretrain/0/20230615-073427/epoch_200.pth
    work_dirs/train_output/cdar/02_ek100/02_D1_D2/03_i3d/011_cop_pretrain/default/default/17934__02_D1_D2-i3d-cop_pretrain/0/20230615-122335/epoch_200.pth
    work_dirs/train_output/cdar/02_ek100/03_D3_D2/03_i3d/011_cop_pretrain/default/default/17935__03_D3_D2-i3d-cop_pretrain/0/20230615-143126/epoch_200.pth
    work_dirs/train_output/cdar/02_ek100/04_D1_D3/03_i3d/011_cop_pretrain/default/default/17936__04_D1_D3-i3d-cop_pretrain/0/20230615-191644/epoch_200.pth
    work_dirs/train_output/cdar/02_ek100/05_D2_D3/03_i3d/011_cop_pretrain/default/default/17937__05_D2_D3-i3d-cop_pretrain/0/20230615-212435/epoch_200.pth
)
idx={{ idx }}
ckpt="${ckpts[$idx]}"

N=$SLURM_GPUS_ON_NODE
calibed_lr="$(perl -le "print $lr * $N / 4")"

tar -xvf data/tarfiles/median-ek100-mm-sada-split.tar -C /data2/local_datasets/
ln -s /data2/local_datasets/median /local_datasets/median

OMP_NUM_THREADS=${N} MKL_NUM_THREADS=${N} torchrun --nproc_per_node="${N}" --master_port=$((10000+RANDOM%20000)) tools/train.py $config \
    --launcher pytorch --seed "$SLURM_ARRAY_TASK_ID" \
    --work-dir "$workdir" \
    --cfg-options \
        optimizer.lr="$calibed_lr" \
        log_config.interval=10 \
        load_from="$ckpt" \
    --validate --test-last --test-best

exit 0
